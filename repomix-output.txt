This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-30T23:53:18.559Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
.github/
  workflows/
    release.yml
.gitignore
action.yaml
Dockerfile
main.py
README.md
run.sh

================================================================
Repository Files
================================================================

================
File: .github/workflows/release.yml
================
name: Build and Push Docker Image

on:
  push:
    branches:
      - main

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,format=long

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

================
File: .gitignore
================
.idea/
initrd
kernel
OVMF.fd
output/

================
File: action.yaml
================
name: Tinfoil Private Inference Build Action
description: Builds, attests, and publishes a private inference deployment.

author: Tinfoil

inputs:
  model:
    description: "OLLAMA model"
    required: true
  domain:
    description: "TLS domain"
    required: true
  github-token:
    description: "GitHub token"
    required: true

  cpus:
    description: "Number of vCPUs"
    default: "16"
  inference-image-release:
    description: "Inference release"
    default: "0.0.6"
  ovmf-release:
    description: "OVMF release"
    default: "0.0.2"

runs:
  using: "composite"
  steps:
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Create deployment manifest
      shell: bash
      run: |
        docker run --rm \
          -v $(pwd)/output:/output \
          -e INFERENCE_IMAGE_VERSION=${{ inputs.inference-image-release }} \
          -e OVMF_VERSION=${{ inputs.ovmf-release }} \
          -e CPUS=${{ inputs.cpus }} \
          -e DOMAIN=${{ inputs.domain }} \
          -e MODEL=${{ inputs.model }} \
          ghcr.io/tinfoilanalytics/pri-build-action:latest

    - name: Attest
      uses: actions/attest@v1
      id: attest
      with:
        subject-path: output/tinfoil-deployment.json
        predicate-type: https://tinfoil.sh/predicate/snp-sev-guest/v1
        predicate-path: output/tinfoil-deployment.json

    - name: Generate release notes
      id: generate-release-notes
      shell: bash
      run: |
        RELEASE_NOTES=$(cat << EOF
        Model: \`$(jq -r .config.model output/tinfoil-deployment.json)\`
        Domain: \`$(jq -r .config.domain output/tinfoil-deployment.json)\`
        SEV-SNP Measurement: \`$(jq -r .measurement output/tinfoil-deployment.json)\`
        Inference Image Version: \`$(jq -r .deployment.inference_image output/tinfoil-deployment.json)\`
        OVMF Version: \`$(jq -r .deployment.ovmf output/tinfoil-deployment.json)\`
        Resources: $(jq -r .deployment.cpus output/tinfoil-deployment.json) vCPUs / $(jq -r .deployment.memory output/tinfoil-deployment.json)GB RAM
        EOF
        )
        echo "release-notes<<EOF" >> "$GITHUB_OUTPUT"
        echo "${RELEASE_NOTES}" >> "$GITHUB_OUTPUT"
        echo "EOF" >> "$GITHUB_OUTPUT"

    - name: Create release
      uses: softprops/action-gh-release@v2
      with:
        files: |
          output/tinfoil-deployment.json
        body: ${{ steps.generate-release-notes.outputs.release-notes }}

================
File: Dockerfile
================
FROM python:3.12-slim

WORKDIR /app

RUN python -m venv /opt/venv

RUN pip install --no-cache-dir sev-snp-measure

COPY main.py /app.py

RUN mkdir -p /output

ENTRYPOINT ["python", "/app.py"]

================
File: main.py
================
import json
import math
import os
import shutil
import urllib.request

from sevsnpmeasure import guest
from sevsnpmeasure.vcpu_types import CPU_SIGS
from sevsnpmeasure.vmm_types import VMMType

INFERENCE_IMAGE_VERSION = os.getenv("INFERENCE_IMAGE_VERSION")
OVMF_VERSION = os.getenv("OVMF_VERSION")
CPUS = int(os.getenv("CPUS"))
DOMAIN = os.getenv("DOMAIN")
MODEL = os.getenv("MODEL")


def model_size(model: str, tag: str) -> int:
    url = f"https://registry.ollama.ai/v2/library/{model}/manifests/{tag}"
    req = urllib.request.urlopen(url)
    body = json.loads(req.read().decode('utf-8'))
    total = body["config"]["size"]
    for layer in body["layers"]:
        total += layer["size"]

    return math.ceil(total / 1000 / 1000 / 1000)


def fetch(url, file):
    print(f"Fetching {url}...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    request = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(request) as response, open(file, 'wb') as out_file:
        shutil.copyfileobj(response, out_file)


fetch(f"https://github.com/tinfoilanalytics/edk2/releases/download/v{OVMF_VERSION}/OVMF.fd", "OVMF.fd")

url = f"https://github.com/tinfoilanalytics/cvmimage/releases/download/v{INFERENCE_IMAGE_VERSION}/tinfoil-inference-v{INFERENCE_IMAGE_VERSION}-manifest.json"
manifest = json.loads(urllib.request.urlopen(url).read().decode('utf-8'))

fetch(f"https://images.tinfoil.sh/cvm/tinfoil-inference-v{INFERENCE_IMAGE_VERSION}.vmlinuz", "kernel")
fetch(f"https://images.tinfoil.sh/cvm/tinfoil-inference-v{INFERENCE_IMAGE_VERSION}.initrd", "initrd")

cmdline = f"readonly=on console=ttyS0 earlyprintk=serial root=/dev/mapper/root roothash={manifest['root']} tinfoil-model={MODEL} tinfoil-domain={DOMAIN}"

print("Measuring...")
ld = guest.snp_calc_launch_digest(
    CPUS, CPU_SIGS["EPYC-v4"], "OVMF.fd", "kernel", "initrd", cmdline,
    0x1, "", VMMType.QEMU, dump_vmsa=False,
)

model_parts = MODEL.split(":")
mem_size = model_size(model_parts[0], model_parts[1]) + 18
mem_size += (mem_size % 2)

deployment_cfg = {
    "config": {
        "model": MODEL,
        "domain": DOMAIN,
    },
    "measurement": ld.hex(),
    "deployment": {
        "cmdline": cmdline,
        "cpus": CPUS,
        "memory": mem_size,
        "inference_image": INFERENCE_IMAGE_VERSION,
        "ovmf": OVMF_VERSION,
    },
    "hashes": manifest,
}

print(deployment_cfg)

with open("/output/tinfoil-deployment.json", "w") as f:
    f.write(json.dumps(deployment_cfg, indent=4))

================
File: README.md
================
# Tinfoil Private Inference Builder

## GitHub Actions Example

```yaml
name: Build and Attest

on:
  push:
    tags:
      - 'v*'

jobs:
  release:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: write
      id-token: write
      attestations: write

    steps:
      - uses: actions/checkout@v4
      - uses: tinfoilanalytics/pri-build-action@main
        with:
          model: deepseek-r1:70b
          domain: inference.delta.tinfoil.sh
          github-token: ${{ secrets.GITHUB_TOKEN }}
```

## Run Locally

```bash
docker run --rm \
    -v $(pwd)/output:/output \
    -e INFERENCE_IMAGE_VERSION=0.0.6 \
    -e OVMF_VERSION=0.0.4 \
    -e CPUS=16 \
    -e DOMAIN=six.delta.tinfoil.sh \
    -e MODEL=deepseek-r1:70b \
    ghcr.io/tinfoilanalytics/pri-build-action
```

================
File: run.sh
================
#!/bin/bash
set -ex

GPU=01:00.0
MEMORY=$(jq -r .deployment.memory tinfoil-deployment.json)000M
CPUS=$(jq -r .deployment.cpus tinfoil-deployment.json)
CMDLINE=$(jq -r .deployment.cmdline tinfoil-deployment.json)
INFERENCE_IMAGE=$(jq -r .deployment.inference_image tinfoil-deployment.json)
OVMF_VERSION=$(jq -r .deployment.ovmf tinfoil-deployment.json)

sudo python3 /shared/nvtrust/host_tools/python/gpu-admin-tools/nvidia_gpu_tools.py --gpu-bdf $GPU --set-cc-mode=on --reset-after-cc-mode-switch
sudo python3 /shared/nvtrust/host_tools/python/gpu-admin-tools/nvidia_gpu_tools.py --gpu-bdf $GPU --query-cc-mode
stty intr ^]
sudo ~/qemu/build/qemu-system-x86_64 \
  -enable-kvm \
  -cpu EPYC-v4 \
  -machine q35 -smp $CPUS,maxcpus=$CPUS \
  -m $MEMORY \
  -no-reboot \
  -bios images/ovmf-v$OVMF_VERSION.fd \
  -drive file=images/tinfoil-inference-v$INFERENCE_IMAGE.raw,if=none,id=disk0,format=raw \
  -device virtio-scsi-pci,id=scsi0,disable-legacy=on,iommu_platform=true \
  -device scsi-hd,drive=disk0 -machine memory-encryption=sev0,vmport=off \
  -object memory-backend-memfd,id=ram1,size=$MEMORY,share=true,prealloc=false \
  -machine memory-backend=ram1 -object sev-snp-guest,id=sev0,policy=0x30000,cbitpos=51,reduced-phys-bits=5,kernel-hashes=on \
  -kernel images/tinfoil-inference-v$INFERENCE_IMAGE.vmlinuz \
  -initrd images/tinfoil-inference-v$INFERENCE_IMAGE.initrd \
  -append "$CMDLINE" \
  -net nic,model=e1000 -net user,hostfwd=tcp::8443-:443 \
  -nographic -monitor pty -monitor unix:monitor,server,nowait \
  -device pcie-root-port,id=pci.1,bus=pcie.0 \
  -device vfio-pci,host=$GPU,bus=pci.1 \
  -fw_cfg name=opt/ovmf/X-PciMmio64Mb,string=262144
stty intr ^c
